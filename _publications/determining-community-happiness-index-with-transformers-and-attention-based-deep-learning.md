---
title: "Determining community happiness index with transformers and attention-based deep learning"
collection: publications
category: manuscripts
permalink: /publication/determining-community-happiness-index-with-transformers-and-attention-based-deep-learning
excerpt: 'Deep Learning'
date: 2023-09-29
venue: 'IAES International Journal of Artificial Intelligence (IJ-AI)'
paperurl: 'https://ijai.iaescore.com/index.php/IJAI/article/view/24030'
citation: 'R. Jayanto, R. Kusumaningrum, and A. Wibowo, “Aspect-based sentiment analysis for hotel reviews using an improved model of long  short-term memory,” IAES International Journal of Artificial Intelligence (IJ-AI), vol. 13, no. 2,  p. 1753-1761, June 2024, doi: 10.26555/ijain.v8i3.691'
---

Abstract:
In the current digital era, evaluating the quality of people's lives and their happiness index is closely related to their expressions and opinions on Twitter social media. Measuring population welfare goes beyond monetary aspects, focusing more on subjective well-being, and sentiment analysis helps evaluate people's perceptions of happiness aspects. Aspect-based sentiment analysis (ABSA) effectively identifies sentiments on predetermined aspects. The previous study has used Word-to-Vector (Word2Vec) and long short-term memory (LSTM) methods with or without attention mechanism (AM) to solve ABSA cases. However, the problem with the previous study is that Word2Vec has the disadvantage of being unable to handle the context of words in a sentence. Therefore, this study will address the problem with bidirectional encoder representations from transformers (BERT), which has the advantage of performing bidirectional training. Bayesian optimization as a hyperparameter tuning technique is used to find the best combination of parameters during the training process. Here we show that BERT-LSTM-AM outperforms the Word2Vec-LSTM-AM model in predicting aspect and sentiment. Furthermore, we found that BERT is the best state-of-the-art embedding technique for representing words in a sentence. Our results demonstrate how BERT as an embedding technique can significantly improve the model performance over Word2Vec.
